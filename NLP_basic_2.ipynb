{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언어 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 시퀀스에 확률을 할당하는 모델\n",
    "크게 통계를 이용한 방법과 인공신경망을 이용한 방법으로 구분\n",
    "\n",
    "ex) 두 문장간 확률 비교\n",
    "    a. 기계 번역\n",
    "    b. 오타 교정\n",
    "    c. 음성 인식\n",
    "\n",
    "ex) 이전 단어로부터 다음 단어 예측\n",
    "\n",
    "\n",
    "하나의 단어 w, 단어 시퀀스 W (n개의 단어:w1,w2,..wn)\n",
    "A. 단어 시퀀스의 확률\n",
    "B. 다음 단어 등장 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 통계적 언어 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     1) 조건부 확률 : chain rule !!\n",
    "     2) 문장에 대한 확률\n",
    "        P(An adorable little boy is spreading smiles)= \n",
    "        P(An)×P(adorable|An)×P(little|An adorable)×P(boy|An adorable little)×P(is|An adorable little boy) ×P(spreading|An adorable little boy is)×P(smiles|An adorable little boy is spreading)\n",
    "     3) 카운트 기반의 접근 \n",
    "            - 한계 : 희소문제(Sparsity problem)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. N-gram 언어 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법\n",
    "  , 앞의 n-1개의 단어만 고려\n",
    "\n",
    "BUT, 한국어에서의 언어모델은 어렵다\n",
    "why? \n",
    "1. 어순이 중요하지 않다\n",
    "2. 교착어이다\n",
    "3. 띄어쓰기를 제대로 안한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 성능비교기준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 플렉서티(perplexity) : 낮을수록 언어모델의 성능의 좋다는 것을 의미함\n",
    "\n",
    "\n",
    "2) 분기 계수(Brancing factor) : 특정 시점에서 평균적으로 몇개의 선택지를 가지고 고민하는지 의미\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다양한 단어 표현 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1) 국소 표현(Local Representation) = 이산표현(Discrete Representation)\n",
    "ex) One-hot-vector, N-gram, Count-based; Bag of Words(DTM)\n",
    "\n",
    "2) 분산표현(Distributed Representation) = 연속표현(Continuous Representation) : 늬앙스 표현 가능\n",
    "ex) Prediction based; Word2Vec(FastText), Count based;Full document(LSA), Windows(glove)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bag of Word(BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 우선, 각 단어에 고유한 정수 인덱스를 부여합니다.\n",
    "\n",
    "(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\Anaconda3\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n"
     ]
    }
   ],
   "source": [
    "##실습 코딩\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "import re  \n",
    "okt=Okt()  \n",
    "\n",
    "token=re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")  \n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업입니다.  \n",
    "token=okt.morphs(token)  \n",
    "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다가 넣습니다.  \n",
    "\n",
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token:  \n",
    "         if voca not in word2index.keys():  \n",
    "             word2index[voca]=len(word2index)  \n",
    "# token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘깁니다.   \n",
    "             bow.insert(len(word2index)-1,1)\n",
    "# BoW 전체에 전부 기본값 1을 넣어줍니다. 단어의 개수는 최소 1개 이상이기 때문입니다.  \n",
    "         else:\n",
    "            index=word2index.get(voca)\n",
    "# 재등장하는 단어의 인덱스를 받아옵니다.\n",
    "            bow[index]=bow[index]+1\n",
    "# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
    "print(word2index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 문서단어행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서로 다른 문서들의 BoW들을 결합한 표현 방법 \n",
    ": DTM, Document-Term Matrix\n",
    "\n",
    "rows = 문서1,2,3,... / cols= 나는, 어제, .. (단어들) / value = bow값\n",
    "-> 한계 : 희소 표현, 단어 빈도 수 기반 접근\n",
    "\n",
    "    \n",
    "TF-IDF(Term Frequency-Inverse Document Frequency) :\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.TF-IDF(Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTM 내에 있는 각 단어에 대한 중요도 계산할 수 있는 가중치\n",
    "\n",
    "    1) 주로 문서의 유사도를 구하는 작업,\n",
    "    2) 검색 시스템에서 검색 결과의 중요도를 정하는 정하는 작업, \n",
    "    3) 문서 내에서 특정 단어의 중요도를 구하는 작업\n",
    "    \n",
    "식) 문서 d, 단어 t, 문서의 총 갯수 n\n",
    "\n",
    "tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수\n",
    "\n",
    "df(t) : 특정 단어 t가 등장한 문서의 수\n",
    "\n",
    "idf(d,t) : df(t)에 반비례 하는 수 log(n/1+df(t)) -> 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단\n",
    "\n",
    "최종식 : tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문서유사도\n",
    "1) 코사인 유사도\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "def cos_sim(A, B):\n",
    "       return dot(A, B)/(norm(A)*norm(B))\n",
    "doc1=np.array([0,1,1,1])\n",
    "doc2=np.array([1,0,1,1])\n",
    "doc3=np.array([2,0,2,2])\n",
    "print(cos_sim(doc1, doc2)) \n",
    "print(cos_sim(doc1, doc3)) \n",
    "print(cos_sim(doc2, doc3))\n",
    "\n",
    "2)유클리드 거리\n",
    "import numpy as np\n",
    "def dist(x,y):   \n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "doc1 = np.array((2,3,0,1))\n",
    "doc2 = np.array((1,2,3,1))\n",
    "doc3 = np.array((2,1,2,2))\n",
    "docQ = np.array((1,1,0,1))\n",
    "\n",
    "print(dist(doc1,docQ))\n",
    "print(dist(doc2,docQ))\n",
    "print(dist(doc3,docQ))\n",
    "\n",
    "3)자카드 유사도(Jaccard similarity)\n",
    "두 개의 비교할 문서를 각각 doc1, doc2라고 했을 때 doc1과 doc2의 문서의 유사도를 구하기 위한 자카드 유사도는 이와 같습니다.\n",
    "즉, 두 문서 doc1, doc2 사이의 자카드 유사도 J(doc1,doc2)는 두 집합의 교집합 크기를 두 집합의 합집합 크기로 나눈 값으로 정의됩니다.\n",
    "J(doc1,doc2)=doc1∩doc2 / doc1∪doc2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본문의 숨겨진 의미 구조를 발견하기 위해 사용\n",
    "\n",
    "1) LSA ( Latent Semantic Analysis )\n",
    "* SVD ( Singular Value Decomposition , 특이값 분해) : A=UΣVT\n",
    "-> 차원 축소 및 잠재적인 의미 포착 기능 \n",
    "-> full SVD와 Truncated SVD(t)로 두 가지로 나뉨\n",
    ": 축소된 U의 각 행은 잠재 의미를 표현하기 위한 수치화 된 각각의 문서 벡터\n",
    ": 축소된 VT의 각 열은 잠재 의미르 표현하기 위해 수치화된 각각의 단어벡터    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
